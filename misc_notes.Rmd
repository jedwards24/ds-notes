---
title: "Misc Notes"
output: html_document
---

This is a place for collecting links and small notes for which there isn't yet an appropriate document in which to put them.

## Functions to look at or use

+ `anyDuplicated()` - test for duplicated rows in selected columns in a data frame. 

`?rc.settings` for Rs autocomplete options e.g. 

## Tabulation functions

`table()` produces contingency table objects.  `addMargins()` and `prop.table()` work with table objects. Also `tabulate()` returns a named vector but only works with numeric or factor inputs. I wrote the following to get a table with vector object. Not sure if it is needed/useful.

```
vec_count <- function(x) {
  tab <- table(x)
  counts <- as.vector(tab)
  names(counts) <- names(tab)
  counts
}
```

## Comparing Two Data Frames

I often want to test data frames for equality, for example when testing that a change to data processing code has not changed the output. If it has changed then I would want to track down the differences. Another application is to look for changes in a dataset.

This [blog post](https://sharla.party/post/comparing-two-dfs/) looks at packages for comparing columns' names and types. There is a summary table of four options in the blog post are: `dplyr::all_equal()`, `janitor::compare_df_cols()`, `vetr::alike()`, `diffdf::diffdf()`. The harder problem is displaying changes to the values in the rows.

I found `all_equal()` to be quite useful. By default it ignores column and row order. Optionally it can convert similar classes (double/integer and character/factor). I found the output didn't help much when many rows were different, which happened mostly when there were small numeric difference between values. I would like an option to ignore a given tolerance. Also it is "questioning" as of version 1.0.0.

There is a newer tidyverse package [waldo](https://www.tidyverse.org/blog/2020/10/waldo/) to compare objects but with improved messaging over `testthat::expect_equal()`. I will try this but still perhaps not ideal the larger data frames I've been using.

The `visdat::vis_compare()` function visualises difference and so is good for getting an overall view when there are lots of differences. I could then go into more detail on individual columns. When there were many columns, I found it hard to tell which columns the differences belonged to and so I would prefer table output usually. Looking into the function the main comparison was done using this code:

```{r, eval = FALSE}
v_identical <- Vectorize(identical)
purrr::map2_df(df1, df2, v_identical)
```

So it is testing for pairwise identical which loses the order-ignoring options of `all_equal`.

Other packages that do the comparison in more detail with are `compare_df` from [compareDF package](https://github.com/alexsanjoseph/compareDF) (gives HTML output) and `arsenal::comparedf()` [vignette](https://cran.r-project.org/web/packages/arsenal/vignettes/comparedf.html).

## A/B Testing

[A/B testing links](http://www.win-vector.com/blog/2015/06/designing-ab-tests/)

[Multiworld testing](https://www.microsoft.com/en-us/research/project/multi-world-testing-mwt/). Bandit-based (?) alternative to A/B testing from Microsoft.

[Why Multi-armed Bandit Algorithm is Not “Better” than A/B Testing](https://vwo.com/blog/multi-armed-bandit-algorithm/) Poor straw man argument against bandit use, but I am saving it because I suspect it is widely read. There is a commentary on this in [Why Multi-armed Bandit algorithms are superior to A/B testing](https://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html), but also see the later [Don't use bandit algorithms](https://www.chrisstucchio.com/blog/2015/dont_use_bandits.html) by the same author. 

## Elbow Points in Curves

Also called knee points. These can be used in situations of diminishing returns e.g. clustering, model fit with increasing complexity, or algorithm tuning. 

https://raghavan.usc.edu//papers/kneedle-simplex11.pdf
https://www.kaggle.com/kevinarvai/knee-elbow-point-detection

## Goodness-of-fit Tests

This concerns testing the fit of data against a distribution. I will only give notes about the chi-squared test here, but there other tests, such as the Kolmogorov–Smirnov test and the Cramér–von Mises criterion.

The chi-squared test usually refers to Pearson's chi-squared test, but this is part of a wider family of hypothesis tests based on chi-squared test statistics. Pearson's test is used to test for goodness of fit, homogeneity, and independence. See [wiki](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test) for basic details. 

I wanted to do a goodness-of-fit test (for Poisson fit). This can be done in base R using the `chisq.test()` function, but how to use it wasn't immediately obvious to me. The test statistic is found by comparing observed bin counts with the expected bin counts under the null (here a Poisson distribution with rate parameter given by the mean of the data). The formula for $k$ bins is $\sum^k_{i=1}(\frac{(O_i - E_i)^2}{E_i})$. Asymtoptically this will have a chi-squared distribution with $k-p-1$ degrees of freedom ($p$ the number of estimated parameters). This choice of number and location of the bins is an open question. The main aim in the choice is to satisfy the assumption of normality in the bin counts, which leads to the chi_squared test statistic. This leads to guidelines that the expected counts should (mostly) be at least 5. 

An alternative, which avoids this problem is to use Monte Carlo simulation, and this is implemented in `chisq.test()`. The observed counts are given as `x` and the expected probabilities as `p`. The test does not need to know what bins these counts represent. The `rescale.p=TRUE` argument will rescale  `p` to sum to one. Using these inputs will use the chi-squared test, but setting `simulate.p.value=TRUE` will instead use simulation. Samples are drawn under the null and the usual test statistic calculated. This gives a distribution of the test statistic under the null, and the observed test statistic can be located in this distribution to give a p-value.

There are functions in the tidymodels package infer `chisq_test()` and `chisq_stat()` which replicate `chisq.test()` in a tidy setting, but I had problems getting these to work for goodness-of-fit tests.

The code below gives an example pipeline for multiple tests using a nested data frame. The data `allw` has counts for each of week 1:51 over several years (columns `year`, `week`, `n`). I wanted to perform a separate test for each year.

```{r, eval=FALSE}
# Helper. Produces a table of probabilities under the null Poisson hypothesis.
# A row is given for each value from 0:max_n, and one for [max_n + 1: Inf).
get_pois <- function(df, max_n, rate) {
  count(df, n, name = "actual") %>% 
    complete(n = 0:max_n, fill = list(actual = 0)) %>% 
    mutate(dpois = dpois(n, rate)) %>% 
    add_row(n = max_n + 1, actual = 0, dpois = ppois(n - 1, rate, F))  
}

max_n <- max(all_w$n)
nested <- nest(allw, data = c(week, n)) %>% 
  mutate(weekly_mean = unlist(map(data, ~(mean(.$n))))) %>% 
  mutate(weekly_var = unlist(map(data, ~(var(.$n))))) %>% 
  mutate(pois_data = map2(.x = data, .y = weekly_mean, ~get_pois(df = .x, max_n = max_n, rate = .y))) %>% 
  mutate(chisq = map(.x = pois_data, 
                     ~chisq.test(x = .x$actual, p=.x$dpois, rescale.p=T, simulate.p.value=TRUE))) %>% 
  mutate(p_val = unlist(map(chisq, ~.$p.value)))
```

## Misc Links

[http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/](Balancing classifiers)

[Scalable bootstrap video](https://www.youtube.com/watch?v=9v6cMnFNiOI&index=1&list=PLi_-RNsPXDTKXADPyrSVAgAhtiIkV04hY) with [slides](https://www.rss.org.uk/Images/PDF/events/2017/M_Jordan%20260417.ppt). Michael Jordan

https://data36.com/data-coding-101-introduction-bash/. Intro to bash coding.

[Measuring Bernoulli Probabilities in the Presence of Delayed Reactions](https://www.chrisstucchio.com/blog/2016/delayed_reactions.html)

[Functional sequences using magrittr](https://blog.rstudio.com/2014/12/01/magrittr-1-5/). You can use `myfun <- . %>% ...` to create a function-like *functional sequence*.

[Difference between print(), cat(), message(), warning(), stop()](https://stackoverflow.com/questions/36699272/why-is-message-a-better-choice-than-print-in-r-for-writing-a-package). Output from `message()` is treated differently from print/cat and so can be identified as a message for error checking. Also displayed in a different colour.

[Hash tables in R](https://jeffreyhorner.tumblr.com/post/117059271933/hash-table-performance-in-r-part-iv). Four part blog series looks at hashing implementations in R and finds best performance using environments. There is a [hash package](https://cran.r-project.org/web/packages/hash/index.html). 

[Some methods for testing for equality of all elements of a numeric vector](https://stackoverflow.com/questions/4752275/test-for-equality-among-all-elements-of-a-single-vector). I looked into this with functional dependencies.

**Partial unlisting**. `unlist()` has `recursive` and `use.names` arguments. `recursive = FALSE` is useful if you only want to remove the first level of the list. See `purrr::flatten()` for alternative (type stable).

**Style for table captions**. There wasn't a clear answer to whether to use title or sentence case in tabel/plot captions except to be consistent. Perhaps sentence case in captions and title case in titles?

+ [Full stops at end of captions](https://english.stackexchange.com/questions/121551/caption-text-punctuation-full-stops-always-necessary-at-the-end). Says this depends on house style. One suggestion is to omit for short sentence fragments but use for full sentences. But the same source says this could be trumped by consistency.
+ [Which words to capitalise in a title](https://english.stackexchange.com/questions/14/which-words-in-a-title-should-be-capitalized). 

**Underscores or dashes in file names**. [Blog post](https://blog.codinghorror.com/of-spaces-underscores-and-dashes/). Most suggest using dashes now since Google recognises this as a word separator while underscores do not split words. This matches how Regex and operating systems generally see things. [This article](https://medium.com/@maddendan/hyphens-vs-underscores-in-file-names-295025782912) suggests using both to leverage this behaviour by using underscores within word groupings and dashes between groupings. This makes sense but it still felt natural to me to use the opposite in e.g. *my-data_2020-08-01.RDS*. I realise this is because humans see the opposite of the computer - the dash links words while underscores are a stronger separator. I am not sure what to do for the best.

## Importing from SAS with haven

The haven package can read directly from SAS files using `read_sas()`. It attaches some extra attributes from the original data. At the top level there is a `label` attribute which has the dataset name. There may also be attributes for each column: a `format` and a `label`. These can be removed by using `zap_formats()` and `zap_label()` directly on the tibble. The function `zap_labels()` are for _value_ labels which are attached to individual values in a vector. Haven reads these columns in as vector of a new class `labelled`. See `semantics` vignette for details. There is also a `zap_missing()` function which replaces empty strings with `NA`. This takes vectors as an input so for a tibble use `mutate_if(is_character, zap_missing)`. This does the same as using `na_if()` but _might_ be faster.

## Exploring Package Dependencies

The `available.packages()` function returns a matrix of information about available packages whether installed or not (I think all on CRAN by default). There are columns `Depends`, `Imports` and `Suggests` so using `x["ggplot2","Depends"]` will give the dependencies for ggplot2. The output is a string. I would like recursive imports for this, but can't find obvious functions to do this.

`tools:::package.dependencies()` is suggested lots but it is depreciated (although still usable). Package miniCRAN has functions but these are based on `package.dependencies()`. There is `renv::dependencies()` but this is used to check code for dependencies by looking for `library()` etc. calls.

[cranly](https://cran.r-project.org/web/packages/cranly/index.html) looks like the best solution. It does the cleaning of the `available.packages()` table and also adds tools for visualising networks. See vignettes but `clean_CRAN_db() %>% build_network(perspective = "package")` will give me the cleaned package table that I am after. Most of the functions operate on a package network built by `build_network(package_table, perspective = "package")`.


