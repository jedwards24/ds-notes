---
title: "Model Notes"
output: html_document
---

These are notes/links relating to comparisons of model fitting algorithms for classification and regression. 

Might be very general until I decide where these should go.

## Formulas and Model Inputs

This [lazyeval vignette](https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html) describes formulas as “language” objects (unevaluated expression) of class "formula" which also store the environment in which they were created (as attributes).

They can be created from using `formula()`, `as.formula()`, `reformulate()`, and using the `~` operator (see [this thread](https://stackoverflow.com/questions/4951442/formula-with-dynamic-number-of-variables) on creating formulas from dynamic variables). The `~` is a kind of quoting operator that delays evaluation. It is used beyond defining formulas, for example in tidy evaluation in packages like purrr.

The structure of a formula differs depending on whether it is one-sided or two-sided. One sided:

```{r eval = TRUE, echo = TRUE}
f <- ~x + log(x)
f[[1]]
f[[2]]
```

Two-sided:

```{r eval = TRUE, echo = TRUE}
g <- y ~ x + z
g[[1]]
g[[2]]
g[[3]]
```
 
The rlang package has `f_rhs()`, `f_lhs()` to simplify this. Note the returned parts are language objects, not strings. `rlang::f_text()` will return the RHS as a string.

Formulas are commonly used in R as inputs to modelling functions, together with a data frame, to generate design matrices. When used this way the `.` can be used to indicate all columns not otherwise in the formula.

Max Kuhn goes into detail with how `lm()` uses the formula and other arguments in [R formula method: good parts](https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/). 

The `model.frame()` functions returns a data frame containing the variables used in the formula. A `subset` argument can be used to select included row, using either an index or a condition on the variables.

```{r, echo = T, eval = TRUE}
model.frame(mpg ~ disp + log(hp), data = mtcars, subset = cyl == 6) 
```

The model frame adds a new attribute `terms`. "The `terms` object contains the data that defines the relationships between variables in the formulas, as well as any transformations of the individual predictors". It "...will be used to generate design matrices on new data". In `lm()` the design matrix is created with `model.matrix()` using terms and the data. The `"factor"` attribute of `terms` is a matrix with a row for each predictor in the formula and a column for each model term (this is mainly interesting when there are interactions).

See `?terms` and `?terms.object`.

Operations on variables included in the formula can only be on single columns (plus simple interactions).

### Retrieving variable names from a formula

* `all.vars(fmla)` gets the variable names in the formula as a character vector but retains the `.` and does not indicate which side of the `~` the variables are on. it ignores operations on variables in the formula.  
* `terms(fmla)` returns more information about the formula. If the formula contains a `.` term then will need to supply a data argument as well: `terms(fmla, data = dt)`.  
* `labels(terms(fmla, data = dt))` will return all variable names in the RHS of the formula as a character vector. This comes from the term's attributes (`term.labels`).
* See above for subsetting to get parts of a formula. The most robust way is to use rlang functions. The formula.tools package contains similar functions. 

### Resources

Max Kuhn has a a pair of posts on the R formula method which motivates what was done in the carat and parsnip packages. [R formula method: good parts](https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/) and [R formula method: bad parts](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/).

From the R core team (2003): [How To Write Model-Fitting Functions in R](https://developer.r-project.org/model-fitting-functions.html)

## Tidymodels

[Tidymodels] is a collection of packages to give a framework for modelling/machine learning. It is built on ideas from the carat package which is no longer developed.

Links to guides:

+ [Blog post](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/).
+ [Rstudioconf workshop](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/).
+ [R Views gentle intro](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/).
+ [tidymodels site](https://www.tidymodels.org/). Contains a long tutorial and many guides. [Github](https://github.com/tidymodels).

This [critique of tidymodels](https://staffblogs.le.ac.uk/teachingr/2020/10/05/on-not-using-tidymodels/) does make some good points and I have similar reservations. I worry that it is just adding an extra layer on top of standard processes and trying to be over-general. The criticisms don't apply equally to the whole of tidymodels. The parsnip and recipes packages are the ones that I have the main doubts about. However, there are useful ideas here so they may be useful. Hopefully other packages can be used easily without buying into the full system.

[K-means with tidymodels](https://www.tidymodels.org/learn/statistics/k-means/) - a small example of using tidymodels (just broom I think) with K-means. Uses `augment()`, `tidy()`, and `glance()` to handle the output and uses map-unnest to neatly run over different numbers of cluster centres.

Main packages (there are many more):

+ rsample - data splitting and resampling.
+ parsnip - fit models.
+ recipes - data preprocessing.
+ workflows - bundle pre-processing, modeling, and post-processing together.
+ tune - tuning.
+ yardstick - model performance metrics.
+ broom - converts model information into tibbles.
+ dials - manages tuning parameters and grids.

Possibly useful is the [pixiedust](https://github.com/nutterb/pixiedust) package which customises table output from models after tidying with broom. 

There is also HW's [modelr](https://github.com/tidyverse/modelr) which performs many of these functions from within the tidyverse.

See also vtreat for some similar ideas.

Quick thoughts on pre-processing data. I want a way of saving the pro-processing steps. A script could be fine but it isn't modular. Functions seem the obvious way but then should I save the arguments? The recipes way is to save an object bundling the data, model formula, and further transformations. This does make sense, but would I rather these were kept separate?

## Clustering

Very quick rough notes on K-means in R.

There is a `kmeans()` function in base R. The data needs to be scaled before using K-means. `scale()` can be used for this. It scales so that mean is 0 and sd is 1.

Some packages that help: `useful`, `cluster`, `factoextra`,  

Methods for choosing numbers of clusters (see [here](https://uc-r.github.io/kmeans_clustering) for first 3):

+ Elbow
+ Silouette
+ Gap
+ Hartigan - similar to elbow??

Some other clustering methods: PAM (medoid-based), heirarchial. K-means doesn't handle categorical data and is sensitive to outliers.

[Cluster Analysis free chapters](https://d396qusza40orc.cloudfront.net/clusteranalysis/Han_Data%20Mining%203e_Chapters%202%2C10%2C11%2C13.pdf). These free chapters accompanied a Coursera clustering course that I looked at. 

[FeatureImpCluster](https://github.com/o1iv3r/FeatureImpCluster). Feature Importance in clustering.
