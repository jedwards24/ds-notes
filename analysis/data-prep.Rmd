# Data Preparation

## Importing & Exporting Data in R

### Base methods

Quick guide http://www.statmethods.net/input/importingdata.html

Comprehensive https://www.datacamp.com/community/tutorials/r-data-import-tutorial#gs.MWOdcL8

The basic function is `read.table()`. There are variants like `read.csv()` and `read.delim()` but these can all be done through `read.table()` with appropriate arguments.

You can import from the clipboard using `read.table("clipboard")`.  

Files can be downloaded from the web using `download.file(url, destfile, ...)`. The `destfile` is where to save. 

### Reading text from a text file

Can use `read.table()` for this but this reads it in as a data frame. To get raw text use either `readLines()` or `scan()`. See http://en.wikibooks.org/wiki/R_Programming/Text_Processing#Reading_and_writing_text_files

### readr package

For reading in data.  The first is used for csv, tab separated etc. and table formatted files. Are faster than base functions, have simpler arguments, and give more useful error messages.

readr supports seven file formats with seven read_ functions:

+	`read_csv()`: comma separated (CSV) files
+	`read_tsv()`: tab separated files
+	`read_delim()`: general delimited files
+	`read_fwf()`: fixed width files
+	`read_table()`: tabular files where columns are separated by white-space.
+	`read_log()`: web log files

Notes:

+ `read_csv()` never reads in as integers unless specified [see news](https://github.com/tidyverse/readr/blob/master/NEWS.md#integer-column-guessing)
+ `read_csv()` read “-” as “\x96” while fread seemed correct. This can be fixed using argument `locale = readr::locale(encoding = "windows-1252")`. See [github issue](https://github.com/tidyverse/readr/issues/892) and `?readr::guess_encoding`.

### Other Packages

[The rio package](https://github.com/leeper/rio) gives a simple `import()` function which will use a different method depending on the file extension. It calls other packages such as data.table, haven, or readxl to do this.

### Writing to Excel

I have had problems with this in the past due to Java dependency issues for the packages I tried. However, when I revisited it recently (2020) it worked fine. The package I used was [openxlsx](https://cran.r-project.org/web/packages/openxlsx/). Another package which might be useful is [writexl](https://github.com/ropensci/writexl). I haven't tried this but it appears to be lightweight (without Excel dependency). It is probably able to only write basic data to the spreadsheet though (e.g. no multiple sheets).

The basic function in the openxl package is `write.xlsx(x, file)`. Multiple sheets can be written by supplying a named list as `x`. Alternatively a workbook can be built up in detail using `createWorkbook()` and then adding layers. See the vignettes.

For an alternative to default Excel column widths (can be bit wide with auto settings) see this 
[thread](https://stackoverflow.com/questions/45860085/r-autofit-excel-column-width/45860216#45860216).

### XML

I've only done a bit on this. The package to use is [xml2](https://github.com/r-lib/xml2). The guide here was useful: [quick guide to XML in R](https://lecy.github.io/Open-Data-for-Nonprofit-Research/Quick_Guide_to_XML_in_R.html).

I am unclear on the structure for XML  but it seems to based on ideas from relational databases with a nested tag structure (thinking of trees might be helpful). Extracting is done with string pattern matching and searching up and down the hierarchy (parents and children).


## Weight of Evidence

These are univariate measures used for exploration of variables and for variable selection. It was hard to find much formal on them, but they seem to be measures used in credit scoring. The best information on them I found is this [Stitchfix  post](https://multithreaded.stitchfix.com/blog/2015/08/13/weight-of-evidence/). The author has a package: [Information](https://github.com/klarsen1/Information).

For a binary response, WoE for level $i$ is the log of $p_i / (1 - p_i) \times  (1 - p) / p$, where $p_i$ is the mean response for level $i$ and $p$ is the mean response for the whole variable. Therefore this is the log odds plus a constant of $log [(1 - p) / p]$ (a logit so 0 when $p=0.5$). So the WoE adjusts the log-odds to take account of imbalanced classes - it measures the ratio of the factor level odds to the variable odds. 

The IV of a variable is a weighted sum of the WoE values for the levels. The weights adjust to give larger weights to levels with more observation and to ensure that all levels contribute a positive value. There is a table that says whether a variable has predictive value based on IV. This is just a heuristic though. The main issue I can see is that it is independent of the amount of data you have. I have also heard it said that it biases towards variables with more levels.

It might be interesting to relate log odds, WoE, relative risk and odds ratio to each other.

## Misc

[Preparing data for classifiers](http://www.win-vector.com/blog/2014/12/a-comment-on-preparing-data-for-classifiers/). John Mount. I have downloaded the paper mentioned here.

[Variable Importance](https://win-vector.com/2018/12/17/vtreat-variable-importance/). Suggests that variable screening (before passing to model fitting) is important. Wide data can cause any algorithm to fail. The vtreat package has several functions to value the variables individually (using linear, piecewise linear, or knn fits).

[Advanced tricks with data table](https://brooksandrew.github.io/simpleblog/articles/advanced-data-table/)

[Speed Comparison for basic row/column operation](http://www.win-vector.com/blog/2019/05/timing-working-with-a-row-or-a-column-from-a-data-frame/). dplyr can be quite a bit slower than base R once the data frame gets to a decent size.

The [variablekey vignette](https://cran.r-project.org/web/packages/kutils/vignettes/variablekey.pdf) in the kutils package gives a method for recording changes in variables in data frames. It is based on a table of old/new variable names/classes/values (where the number of distinct values is not too large). The ordering of the values can be recorded. The table can be changed in, e.g. excel, then these changes can be applied to the data frame. I like the general idea of recording and displaying changes but I want to only make changes via R code.
