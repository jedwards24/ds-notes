# Logistic Regression

## Logistic Regression Options in R

I didn't try many options: `glm()` in base, `glmnet` and `speedglm`. Due to separability (see below) issues I found I needed some regularisation which only `glmnet` provided. The base `glm()` gave output with a warning where the data was separable but `speedglm` gave no output. I've not looked into Bayesian versions but they do exist. 

There is an ordinal logistic regression in the `MASS` package.

## Separation

There is an issue that can occur in logistic regression when the data is perfectly separable i.e. there are parts of the data that can explained perfectly. This causes infinite likelihood for multiple models which can cause coefficients to be unstable and very large. It can solved by some form of regularisation (e.g. ridge, lasso, Bayes). There is a discussion [here](https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression).

More detail on the subject is given in a [thesis by Konis (2007)](https://ora.ox.ac.uk/objects/uuid:8f9ee0d0-d78e-4101-9ab4-f9cbceed2a2a). This is accompanied by an R package `safeBinaryRegression` which checks for separation before fitting. The function `detect_separation` from `brglm2` does similar.

The standard categorisation of separation seems to come from Albert & Anderson (1984) who give three classes of datasets: completely separated (CS), quasi-completely separated (QCS), and overlapped. The data is CS if there is hyperplane such that all the 1s are on one side and all the 0s on the other. QCS is similar but some 0s and 1s lie on the separating hyperplane. For both CS and QCS the maximum likelihood for the regression coefficients is infinite. In CS these solutions are non-unique, in CQS it may be unique.

## Misc Notes

Agresti (section 5.1.4) discusses the situation where X rather than Y is random, such as in retrospective studies. Odds ratios and hence LR can be used in these and it gives a Bayesian justification. It notes that a LR always holds in the situation $Y=i$ and $X\sim N(\mu_i, \sigma^2)$. If the variances differ then the model has a quadratic term and a non-monotone relationship.  

Agresti (section 6.5.3) has a short bit on sample size determination in LR. It gives two refs: Hsieh et al. (1998) and Whittemore (1981). I found a later 1998 by Hsieh which references the others. It is in the Papers >> GLMs folder.

The base r `glm()` has a `predict.glm()` (can just use `predict()`) that can be used to get responses and se for new data for a fitted model. Example code use is `predict(fit, newdata = data.frame(x = p_range), type = "link", se.fit = T)`. The `type = "link"` is the default and output on the logit scale, `type = "response"` is on the response scale (i.e. in [0,1]), and `type ="terms"` is on the logit scale but centred.

**Prediction uncertainty**. The following is for linear regression so will need to be transformed) The covariance matrix of parameters is $\Sigma = \sigma^2(\mathbf{X}^\intercal\mathbf{X})^{-1}$. The mean response $\hat y_i=\mathbf{x}_i\mathbf{\hat b}$ which has variance $\mathbf{x}_i\Sigma\mathbf{x}_i^\intercal$.

[http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/](A simpler derivation of logistic regression)

## Books and Resources

I've tried:

+ Agresti's Categorical Data Analysis
+ Extending the Linear Model with R by Julian J. Faraway

## Output Analysis

Agresti (section 5.1.2) has a bit on this. For categories it suggests using the sample logit $log[y_i+0.5/(n_i-y_i+0.5)]$ for each category $i$ ($y_i$ is the number of successes). The 0.5s reduce the bias for small $n,y$ to give the least biased estimator. For continuous variables it suggests using a smoothing mechanism such as a GAM. This replaces the linear predictor of a GLM by a smooth function - the plot should be S-shaped.

#### Confidence Intervals

[This thread](https://stackoverflow.com/questions/39750965/confidence-intervals-for-ridge-regression) says that `glmnet` does not give standard errors for prediction probabilities. This [Casella paper](http://www.stat.ufl.edu/archived/casella/Papers/BL-Final.pdf) and a [vignette for the penalised package](https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf) (page 18) explain that they aren't meaningful for biased estimates (e.g. penalised regression). A bootstrap method can be used but it only gives an estimate of the variance of the estimates but this doesn't include any bias estimate. Some code is given [here](https://www.reddit.com/r/statistics/comments/1vg8k0/standard_errors_in_glmnet/) to calculate an estimate of se. It says that the estimates are biased and worse for Lasso than Ridge.

There are more links and discussion in [this thread](https://stats.stackexchange.com/questions/91462/standard-errors-for-lasso-prediction-using-r) (includes more on the Bayesian Lasso).

The Casella paper linked earlier has more detail on Bayesian options. A Gibbs sampler is used to get uncertainty estimates, which I assume is not practical for my problem. Note that this is for standard error estimates of coefficients while I only need estimates for the predictor. There is a part illustrating how bootstrap estimates for lasso coefficients are not reliable. 


