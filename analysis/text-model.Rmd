# Text Modelling

Also see `string_notes.Rmd` for text processing.

## Natural Language Processing (NLP)

I will use NLP as a broad umbrella for anything relating to getting information from text. It also refers to handling other forms of language such as speech. Text will vary in how structured it is.

Initial notes are from some general articles:

+ https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1
+ Coursera course [Natural Language Processing from HSE](https://www.coursera.org/learn/language-processing).

Some algorithms/techniques used in the area:

+ Bag of words. Counts occurences of each word in text, discarding grammer and word order.
+ Term Frequency — Inverse Document Frequency (TFIDF). Weights word importance in texts relative to their frequency in the whole corpus. This helps handle common "unimportant" words such as "the" and "and".
+ Tokenization. Segmenting running text into pieces (tokens) e.g. sentences and words. This might just mean splitting on spaces and removing punctuation, but can be more complex too.
+ Stop words. These are lists of common basic words to remove. 
+ Stemming. Slicing the end or the beginning of words with the intention of removing affixes. This groups similar words together e.g. playing, player. May be done heuristically.
+ Lemmatization. Aims to reduce words to their base form and group together different forms of the same word e.g. remove tenses and unify synonyms (best -> good, went -> go).
+ Topic Modelling. Clusters texts based on latent topics. A common technique is Latent Dirichlet Allocation (LDA). This is an unsupervised learning method based on probabilities that each word belongs to a topic, and that the document will be generated by the topic. The ouput is to define each document and a proportion of each topic.

R specific guides:

+ [Text Mining with R](https://www.tidytextmining.com/). Based around tidytext.
+ [An Introduction to Text Processing and Analysis with R](https://m-clark.github.io/text-analysis-with-R/)

R packages:

+ [tidytext](https://cran.r-project.org/web/packages/tidytext/index.html). Text analysis in a tidy format. Has an online book so well documented. Also see vignettes.
+ [lexicon](https://cran.r-project.org/web/packages/lexicon/index.html). Lexical hash tables and dictionaries.
+ [corpus](https://cran.r-project.org/web/packages/corpus/). General text functions.
+ [hunspell](https://cran.r-project.org/web/packages/hunspell/index.html). "Stemmer, tokenizer, and spell checker.
+ [textclean](https://github.com/trinker/textclean). Tools for cleaning and normalising text.

Other resources:

+ [wordnet](https://wordnet.princeton.edu/). A large lexical database for English.
+ [NLP Cousera course](https://www.coursera.org/learn/language-processing) from the National Research University - Higher School of Economics. Part of a [advanced ML specialisation](https://www.coursera.org/specializations/aml#courses).
+ 

### Notes from HSE Course

From [NLP Cousera course](https://www.coursera.org/learn/language-processing). Some parts are low on detail since I wasn't equally interested in all material.

**Week 1 - Intro & text classification**

Tokens can be words or characters or anything else. The first step is convert these to features. The simplest approach is to use *bag of words* (BOW). In the basic version of this each feature is a token. This disregards the ordering of the tokens in the text so further features are often added as *n-grams* (combinations of n consecutive tokens in the text). This causes a rapid increase in the number of features so usually high or low frequency n-grams are usually removed (high frequency likely to have little meaning and less frequent unusual or misspellings).

Each observation for the model is a documents or text out of the *corpus* (collection of documents). The values for each document/feature can be based on *term frequency*, which can be measured in a number of ways:

+ Binary - can the term be found in the document.
+ Count - number of occurrences of term in the document.
+ Adjusted for document size - term count / count of all terms in the document.
+ Log normalised: log(1 + term count).

Commonly used is *term frequency - inverse document frequency* (TF-IDF). IDF is $log(N/|t\in D|)$ where $N$ is number of documents and $|t\in D|$ is the number of documents containing the term. TD-IDF is then $TF * IDF$. This identifies terms that are common in the current document while avoiding commonly used terms.

Once the matrix is formed, further normalise row-wise by dividing by the L2 norm or row sum.

This matrix can then be used as input to the usual model techniques. It is very wide and sparse so tree-based methods can be inefficient, but regression, naive Bayes and SVM work fine.

With many features memory can become an issue in mapping to the features, especially with parallel computing. A trick to reduce memory costs by reducing features is to use a hash. Convert strings to numbers then take $\mod 2^b$ (say $b=20$). This causes *collisions* between features where different terms are mapped to the same hash but it works in practice.

*Word embeddings* map words or phrases to a vector of real numbers which greatly reduces the dimension of the model matrix (say ~300 columns). An example is word2vec which uses a NN to give a mapping from words to a (dense) numeric vector. Mappings for n-grams can be found by summing vectors of component words or by using a 1D convolutional filter. Similar sentences will be recognised as similar by the convolutional filter. Vectors for n-grams are found by using a 1D sliding window, then we use the *max pooling* of this vector. See https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf

Features can be built from individual characters, rather than words. This has been found to work better than BOW on large datasets (e.g. >1M rows).

**Week 2 Language Models**

The aim is to estimate probability of next word given previous words.

An n-gram language model bases probabilities on counts of word sequences in a corpus. These include start and end points to normalise for different possible sequence lengths. A bigram model predicts probabilities of words given only the previous word.

Perplexity is a test of the model (lower better). Test $perplexity = p(w_{test})^{-(1/N)}$ where $N$ is the length of the test corpus. So it is the likelihood of the model generated text adjusted for N. A problem is that a new n-gram in the test set will give infinite perplexity. There are several smoothing techniques to solve this:

* Laplacian smoothing pull some probability from frequent to infrequent n-grams
* Katz backoff
* Interpolation smoothing
* Absolute discounting compares bigram counts between train and test sets.
* Kneserney smoothing uses the diversity of contexts for each word (probably most popular method).

Sequence labelling gives labels to words e.g. noun, name. Part-of-speech (PoS) tagging. One model used to do this a hidden markov model:

* $x$ are words $y$ are labels. Want $\arg\max p(y|x) = \arg\max p(x,y)$.
* Makes a Markovian assumption on sequence of y. p(y_t)=p(y_t|y_t-1). 
* Probability of the word given tag is stationary.

To generate text:
* Generate tag given previous tag.
* Generate word given tag
* The model has $N(N+1) + KT$ parameters ($N$ number of tags, $K$ number of words, $T$ sequence length).
* Solve model using EM algorithm (Baum-Welch specifically).
* Viterbi decoding (dynamic programming) can be used to find the most probable sequence of hidden states.

Named entity recognition models:

* HMM (generative model)
* Maximum entropy Markov model (discriminative model) - text is observed here.  This applies softmax to the dot product of features and weights. This is similar to logistic regression, but the features are complicated.
* Conditional random field (linear chain) (discriminative model - as in MEMM P(y|x)). Based on an undirected graph.  Normalisation is more difficult than in MEMM.  

NN can also be used in sequence labelling. 

Using each word as a distinct feature leads to a model with many dimensions. Also we cannot use similarity between words to improve the model (especially useful with words not seen before). Use distributed representations for words (low dimensional vectors). Models to create this:

* Probabilistic Neural Language Model. This is very complicated with lots of parameters.
* Log-Bilinear Language Model. Far fewer parameters.

Recurrent NN can generate word sequences. A greedy approach picks the highest probability of the next word. Beam search check the n best sequences at each stage.

**Week 3 - Vector Space Models of Semantics**

Semantics is the meaning of words or documents.

* Word and Sentence Embedding
* Distributional Semantics quantifies semantic similarities between linguistic terms.
* Represent words and documents by vectors so that similar words/documents have similar vectors.
* First order co-occurrences - syntagmatic associates - relatedness. Occur together e.g. bee & honey. 
* Second order co-occurrences - paradigmatic parallels - similarity. Interchangeable e.g. bee & bumblebee. 

One method is to use a cosine similarity between vectors of nearby words occurrences for each word - say, a sliding window of fixed size. This will be biased for common words.

A better method is Pointwise Mutual Information (PMI):
* $PMI = log [p(u,v)/(p(u)p(v))] = log[n_{uv}/(n_un_v)]$, where $n_{uv}$ are co-occurrences (?*check this*)
* This is replaced by $\max(0, PMI)$ to remove $-\infty$ output for words not seen.

Distributional hypothesis - “you will know a word by the company it keeps”. Vector space models of semantics use dimension reduction to approximate the PMI matrix. We could also use a PMI matrix of word/context similarity.

Matrix Factorisation:

* Single Value Decomposition (SVD). Gives a matrix as a product of 3 matrices. The first k components of each matrix (respectively first k cols, diagonal elements, rows) give the best approximation of the original matrix measured by the Frobenius norm (element wise root sse). SVD uses squared loss.
* For the semantics model we want decomposition into 2 matrices. Heuristics can be used to convert the 3 into 2.
* Global vectors for word representation (GloVe) uses weighted squared loss - approximated more important elements of the matrix more accurately (more important words).
* Another method to build word embeddings is language modelling. 
* Skip gram model also produces two matrices but is slow.
* Skip gram negative sampling predicts yes/no for word pairs and is more efficient.

*Word2vec* is a software package with several variants:

* Continuous bag of words (CBOW): predicts a word given previous words.
* Continuous skip gram: predicts context words given a word.
* Open source code is at [code.google.com/archive/p/word2vec/](code.google.com/archive/p/word2vec/)

These can be used to get similarities between words. See [https://www.aclweb.org/anthology/Q15-1016/](https://www.aclweb.org/anthology/Q15-1016/) for comparisons of methods (no one best method).

There is also *paragraph2vec* and *doc2vec*. Architecture used are Distributed Memory (DM) and Distributed Bag of Words (DBOW). Predict doc/para similarities.

Word embeddings also have the nice side effect of giving word analogies (man:woman is as king:queen). That is, king - man + woman = queen. The closest word to the sum of vectors king - man + woman is queen. 

However, this isn’t as effective as it seems. The 3 input words are excluded from output. If they aren’t then the output is often e.g. king. So $cos(b-a+a’) \rightarrow b$. This is because $-a+a’$ is often small. Just taking the nearest neighbour of $b$ has 70% accuracy. Word2vec doesn’t do well on analogies when $b$ and $b’$ are not similar.
It does well in inflectional morphology examples but not in derivational morphology examples (bake:baker, edit:editable, home:homeless). Also poor is lexical semantics, with exceptions for gender and countries, which do improve on the baseline model. See [https://www.aclweb.org/anthology/S17-1001/](https://www.aclweb.org/anthology/S17-1001/).

Using characters:

* English uses prefixes to change the meaning of words (morphology).
* [https://arxiv.org/abs/1706.00377](https://arxiv.org/abs/1706.00377) uses linguistic knowledge.
* FastText represents words as a bag of character n-grams. A word is a sum of sub-word vectors.
* Sent2vec represents sentences as a sum of sub-sentence units.
* Starspace (FB research) gives embeddings for labels, documents and words in the same space.

__Topic Modelling__ is a kind of soft biclustering which clusters both words and documents to softly assign words and documents to topics. 

* A topic is a probability distribution of words. Topics are hidden.
* Probabilistic Latent Semantic Analysis (PLSA) 1999 is a basic topic model. Assumes words are conditionally independent of the document (given topic). 
    * Has parameters for: p(word in topic) and p(topic in doc). 
    * Trained using EM algorithm.

Other topic models:

* Latent Dirichlet Allocation (LDA). Similar to PLSA but is Bayesian.
* Graphical models e.g Hierarchical topic models
* Bayesian methods
* Dynamic topic models. Topic probs change over time.
* Multilingual topic models. Same topics over multiple languages.
* ARTM. Combines previous ideas using regularisers.

**Week 4 - Sequence to Sequence Tasks**

Examples are machine translation and summarisation

Machine Translation

* Data requirements: parallel corpora. Some corpora are comparable - not exact translations.
* Evaluation of methods is difficult. 

BLEU is a popular automated technique for measuring similarity. It counts n-gram scores of matches between reference and output and also a brevity score. Translation goes from a source to a target. There are several layers in translation: direct, syntactic, semantic, interlingual.

Translation models want to find Pr(target sentence|source sentence) then choose maximiser. This is $e*=\arg\max p(s|t)p(t)$. 

* $p(t)$ is the fluency of the translation (language model).
* $p(s|t)$ is the adequacy of the translation (translation model). Also the noisy channel.

The argmax search problem is implemented by a decoder.

Translations can have word reordering, one-to-many or many-to-one translations, or words that are only present in one version. Therefore, as well as a word translation model (e.g. matrix of probabilities), we need a word alignment model. Word alignment is a hidden state in the translation model. 

_Encoder-decoder Architecture_ is an NN method. Not specific to translation but important there. An encoder converts a sentence to a vector. A decoder does the reverse. Each is some kind of NN. Putting these together gives a sequence to sequence conversion. An attention mechanism can speed this process up by weighting parts of the sentence that are more important. Gives improvements with long sentences. Softmax is slow to calculate for large vocabulary. Can instead use hierarchical softmax which makes binary decisions on a tree. Out-of-vocabulary (OOV) words are a problem. Some methods: copy mechanism, sub-word modelling, byte-pair encoding (this works well and is commonly used).

Chatbots can be retrieval or generative based. Generative models use sequence to sequence conversion.

_Summarisation_ is generating a short summary of a document.

* An extractive summary uses parts of the original document. 
* An abstractive summary rewrites in a shortened form.
* Can use an encoder-decoder model.

_Simplification_ reduces the lexical and syntactical complexity of text.

* Split, delete, paraphrase.
* Rule-based approach works well.
* NN with Reinforcement learning is an option, but It is not easy to make it work well. The RL rewards simplicity. 

**Week 5 - Dialog Systems**

Task-oriented dialog systems e.g. Apple Siri or chatbots.

_Intent classification_ asks what does the user want. Fit to predefined scenarios. Like a form the user needs to fill in. Each has slots to be filled in. This is a sequence tagging problem - identify start and end of slot to assign words to the slot (slot tagger). Evaluate intent with accuracy measure and slot tagger with F1-measure.

Intent classifier can be done with BOW, RNN, CNN. CNN works well on key-phrase-type datasets. Slot tagger is more difficult. Hand crafted rules don’t scale. Can use cond random fields, RNN, CNN, any seq2seq with attention. Intent and slot can be trained together which gives better results and is faster.

Context classification is needed for multi-turn dialogs. Need memory network. Encode previous utterances into “memory”.

Adding lexicon to NLU
[https://arxiv.org/abs/1511.08308v5](https://arxiv.org/abs/1511.08308v5). 

* Using lexicon datasets to include knowledge not in text e.g. lists of cities. 
* Match n-gram to lexicon entries.
* Use BIOES encoding (begin, inside, outside, end, single). Use these tokens to encode matches and use as features.

Dialog manager tracks the state of the dialog. The state records probability distributions over goals, method, and requested slots. Given the state you need a policy for the response. Can be hand-crafted or RL. Supervised learning is difficult because labelled data is expensive to collect.

