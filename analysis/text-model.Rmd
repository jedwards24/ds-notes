# Text Modelling

Also see `string_notes.Rmd` for text processing.

## Natural Language Processing (NLP)

I will use NLP as a broad umbrella for anything relating to getting information from text. It also refers to handling other forms of language such as speech. Text will vary in how structured it is.

Initial notes are from some general articles:

+ https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1
+ Coursera course [Natural Language Processing from HSE](https://www.coursera.org/learn/language-processing).

Some algorithms/techniques used in the area:

+ Bag of words. Counts occurences of each word in text, discarding grammer and word order.
+ Term Frequency â€” Inverse Document Frequency (TFIDF). Weights word importance in texts relative to their frequency in the whole corpus. This helps handle common "unimportant" words such as "the" and "and".
+ Tokenization. Segmenting running text into pieces (tokens) e.g. sentences and words. This might just mean splitting on spaces and removing punctuation, but can be more complex too.
+ Stop words. These are lists of common basic words to remove. 
+ Stemming. Slicing the end or the beginning of words with the intention of removing affixes. This groups similar words together e.g. playing, player. May be done heuristically.
+ Lemmatization. Aims to reduce words to their base form and group together different forms of the same word e.g. remove tenses and unify synonyms (best -> good, went -> go).
+ Topic Modelling. Clusters texts based on latent topics. A common technique is Latent Dirichlet Allocation (LDA). This is an unsupervised learning method based on probabilities that each word belongs to a topic, and that the document will be generated by the topic. The ouput is to define each document and a proportion of each topic.


R specific guides:

+ [Text Mining with R](https://www.tidytextmining.com/). Based around tidytext.
+ [An Introduction to Text Processing and Analysis with R](https://m-clark.github.io/text-analysis-with-R/)

R packages:

+ [tidytext](https://cran.r-project.org/web/packages/tidytext/index.html). Text analysis in a tidy format. Has an online book so well documented. Also see vignettes.
+ [lexicon](https://cran.r-project.org/web/packages/lexicon/index.html). Lexical hash tables and dictionaries.
+ [corpus](https://cran.r-project.org/web/packages/corpus/). General text functions.
+ [hunspell](https://cran.r-project.org/web/packages/hunspell/index.html). "Stemmer, tokenizer, and spell checker.
+ [textclean](https://github.com/trinker/textclean). Tools for cleaning and normalising text.

Other resources:

+ [wordnet](https://wordnet.princeton.edu/). A large lexical database for English.

### In Practice

Next are notes on doing predictive modelling on labelled data. Notes from the Coursera course. 

Tokens can be words or characters or anything else. The first step is convert these to features. The simplest approach is to use *bag of words* (BOW). In the basic version of this each feature is a token. This disregards the ordering of the tokens in the text so further features are often added as *n-grams* (combinations of n consecutive tokens in the text). This causes a rapid increase in the number of features so usually high or low frequency n-grams are usually removed (high frequency likely to have little meaning and less frequent unusual or misspellings).

Each observation for the model is a documents or text out of the *corpus* (collection of documents). The values for each document/feature can be based on *term frequency*, which can be measured in a number of ways:

+ Binary - can the term be found in the document.
+ Count - number of occurrences of term in the document.
+ Adjusted for document size - term count / count of all terms in the document.
+ Log normalised: log(1 + term count).

Commonly used is *term frequency - inverse document frequency* (TF-IDF). IDF is $log(N/|t\in D|)$ where $N$ is number of documents and $|t\in D|$ is the number of documents containing the term. TD-IDF is then $TF * IDF$. This idnetifies terms that are common in the current document while avoiding commonly used terms.

Once the matrix is formed, further normalise row-wise by dividing by the L2 norm or row sum.

This matrix can then be used as input to the usual model techniques. It is very wide and sparse so tree-based methods can be inefficient, but regression, naive Bayes and SVM work fine.

With many features memory can become an issue in mapping to the features, especially with parallel computing. A trick to reduce memory costs by reducing features is to use a hash. Convert strings to numbers then take $\mod 2^b$ (say $b=20$). This causes *collisions* between features where different terms are mapped to the same hash but it works in practice.

*Word embeddings* map words or phrases to a vector of real numbers which greatly reduces the dimension of the model matrix (say ~300 columns). An example is word2vec which creates a mapping from words. Mappings for n-grams can be found by summing vectors of component words or by using a 1D convolutional filter.

Features can be built from individual characters, rather than words. This has been found to work better than BOW on large datasets (e.g. >1M rows).

