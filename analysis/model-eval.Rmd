# Model Evaluation

## Variable Importance

Records notes on variable importance (VI) measures for random forests. May drift into other evaluation methods.

Brandon Greenwell (who authored the `pdp` package) has an R package `vip` for variable importance measures. It is on [CRAN](https://cran.r-project.org/web/packages/vip/index.html) but there is more information and articles on the [package website](https://koalaverse.github.io/vip/index.html). There is also a [paper](https://arxiv.org/abs/1805.04755) connected to this work. The authors have a [book chapter](https://bradleyboehmke.github.io/HOML/iml.html) on interpretable machine learning which includes VI. The Interpretable Machine Learning book has a [section on variable importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html).

The notes below are taken from these (I won't give exact references in most cases).

Some modelling methods have *model-specific* measures of importance which can only be used with that specific model class, while other methods must rely on *model-agnostic* approaches. 

**TBC**

[glmnet VI](https://stackoverflow.com/questions/35461839/glmnet-variable-importance/39017938)

## Model explanation

[DALEX](https://modeloriented.github.io/DALEX/) and 
[DALEXtra](https://modeloriented.github.io/DALEXtra/index.html)

[lime](https://github.com/thomasp85/lime) Local interpretable model-agnostic interpretations.

## Misc

[Model Explanation System](http://www.blackboxworkshop.org/pdf/Turner2015_MES.pdf) and [MES update](https://arxiv.org/pdf/1606.09517.pdf). Method for explaining complex ML model output.

[Brier Score](https://en.m.wikipedia.org/wiki/Brier_score) This is score function that measures the accuracy of probabilistic predictions for discrete outcomes. Basically a mean squared error.

You can test the calibration of a model by plotting predicted versus empirical probabilities. A method for binning which emphasises tails is given [here](https://stats.stackexchange.com/questions/25482/visualizing-the-calibration-of-predicted-probability-of-a-model).


## Model Validation

I'll put some links here relating to handling test/training data splits (or cross-validation etc.).

+ [Random Test/Train Split is not Always Enough](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/) Time based data might not want a random split.

## Loss Functions

+ [Binary cross-entropy/log loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

## ROC Curves

[R packages for ROC](https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/)

[ICML tutorial on ROC](http://people.cs.bris.ac.uk/~flach/ICML04tutorial//)

[ROC Curve, Lift Chart and Calibration Plot](http://www.stat-d.si/mz/mz3.1/vuk.pdf)

[Multiple ROC curves in ROCR](https://stackoverflow.com/questions/14085281/multiple-roc-curves-in-one-plot-rocr)
I adapted this to use with ggplot. Three methods:
```{r roclines, eval = F}
# There are 2 performance objects perf1 and perf2
#option 1
plot(perf1)
abline(0, 1, lty = 2)
lines(perf2@x.values[[1]], perf2@y.values[[1]], col = 2)

# option 2
plot(perf1)
abline(0, 1, lty = 2)
plot(perf2, add = TRUE, colorize = TRUE)

# ggplot option

rocdt <- tibble(x = c(perf1@x.values[[1]], perf2@x.values[[1]]), 
                y = c(perf1@y.values[[1]], glm = perf2@y.values[[1]]), 
                model = c(rep("A", length(perf1@x.values[[1]])), rep("B", length(perf2@x.values[[1]]))))
ggplot(rocdt, aes(x = x, y = y, col = model)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  xlab("False Positive Rate") +
  ylab("True Positive Rate") +
  theme_minimal()
```

The most common packages in R for ROC curves are `pROC` and `ROCR`. I went with the latter because I found suggestions that it was faster. See an explanation [here](https://stats.stackexchange.com/questions/269552/difference-between-proc-and-rocr-in-compute-time-and-accuracy). Two articles introducing ROC and `ROCR` are: [Illustrated Guide to ROC and AUC](https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/) and [A small intro to the ROCR package](https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/). The second has more detail. The first links to two useful papers by Fawcett and Hand which I've saved in the GLM folder. The Hand article gives a criticism of AUC.

ROC tests a model's ability to classify but does not directly test the accuracy of the responses as it only uses the ordering of the scores. Two models with different predicted probabilities but with the same order will have the same ROC and AUC. 

