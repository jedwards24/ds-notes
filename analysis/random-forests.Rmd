# Random Forests

```{r rf_setup, include = FALSE}
library(tidyverse)
library(kableExtra)
library(edwards)
```

## Links

[Original Breiman paper](https://link.springer.com/article/10.1023/A:1010933404324)

[JSS Ranger paper](https://arxiv.org/abs/1508.04409) - gives good benchmarking and comparison with other packages. Explains how it is faster. The package will have developed further since this. 

[Guide to the original random forest software by Leo Breiman and Adele Cutler](https://www.stat.berkeley.edu/~breiman/RandomForests)

[Slides on RFs and GBMs in R](https://bgreenwell.github.io/MLDay18/MLDay18.html#1) - these are from the `pdp` package author Brandon Greenwell. Useful introduction. 

[Manual for Breiman fortran software](https://www.stat.berkeley.edu/~breiman/Using_random_forests_v4.0.pdf) - may be useful as guide to variable importance and imputation.

## Basics

Random forest predictions are a combination of multiple trees. The observations used to fit each tree are sampled, with replacement, from the data. At each split only a subset (of size `mtry`) of variables are available. This makes makes the trees more dissimilar from each other.

Predictions will tend to overfit to the data, but a better estimate of model fit can be found using *out-of-bag* (OOB) predictions. If an observation was used to grow a given tree then it is said to be *in-bag*. Observations not used are OOB. Just using predictions from trees for which the observation was OOB gives an OOB prediction. A majority vote is used with, I think, ties split randomly.

In ranger, the in-bag status of each observation can be found by using the argument `keep.inbag = TRUE` when growing the forest. This adds an extra element `inbag.counts` to the forest object which is a list of counts of how often an observation was used for each tree (`num.trees` vectors each of length `n`). Predictions for each separate tree can be obtained using `predict(rf, data, predict.all = TRUE)`. The `.$predictions` of the output will then be a matrix rather than a vector with each column corresponding to individual trees.

## Ranger Model Settings

The main inputs are either `formula`/`data`, `x`/`y`, or `data`/`dependent.variable.name`.

Tuning parameters (with defaults):

* `mtry`. Default is floor(sqrt(num independent vars)).
* `max.depth`. Default is unlimited depth.
* `min.node.size`. Default is 1 for classification, 5 for regression, and 10 for probability.

Other parameters:

* `write.forest` defaults to `TRUE`. Needed for predictions but reduces memory use if turned off.
* `splitrule` defaults to “gini” for classification and “variance” for regression.
* `respect.unordered.factors`. Defaults to FALSE (='ignore') but should always be set to TRUE (='order'). Setting to “partition” will be slow.
* `importance` and `local.importance` for VI.
* `oob.error`. Compute OOB prediction error. Set to FALSE to save computation time
* `seed` defaults to generating the seed from R.
* `num.threads` defaults to the number of CPUs available.
* `classification`, and `probability`. Set tree type - see below.

### Tree Type

The type of model fitted depends on the type of the target variable column and the `classification` argument:

* If the target is a factor, logical, or character, then classification is used.
* If the target is numeric, regression is used by default, unless the argument `classification = TRUE` is given, in which case classification is used.

If using classification, a further option is to supply `probability = TRUE`. This changes the predictions from class predictions to probabilities. 

## Ranger Model Output

The fitted model has `prediction.error` element which is OOB error on the training data. For classification this is the proportion of samples misclassified and for regression it is the MSE. For a probability tree it is the Brier score (which equals MSE for 0/1 targets).

There is a `confusion.matrix` element for classification trees (not probability trees). Based on OOB.

Unlike randomForest, the ranger package does not record error measures by number of trees. However, using `keep.inbag = TRUE` this can be done. See `jemodel::rang_oob_err()`.

## Prediction

See `?predict.ranger` for full information.

The `predict.ranger` method needs inputs of a fitted ranger object and new data matching the training data. The data can only be left empty when quantile regression has been used. There is an option to only use the first `num.trees` trees for the prediction.

`predict.ranger` returns a `ranger.prediction` object. For classification/regression this is a list with elements: `predictions`, `num.trees`, `num.independent.variables`, `num.samples`, and `treetype`. 

The `predictions` object takes different forms depending on target variable type and model settings. For default settings:

* Numeric target (regression tree) gives prediction of numeric vector.
* Numeric target (classification tree built with  `classification = TRUE`) gives a numeric vector with values matching the target variable values.
* Factor target (classification tree) gives a factor vector with values matching the target variable values.
* Factor target  with `probability = TRUE` (classification tree) gives a matrix of probabilities with a column for each factor level in the target variable.

This is with the default arguments `type = response` and `predict.all = FALSE`. Other options for type include se and quantiles. If `predict.all = TRUE` then the predictions are given for each individual tree which adds a dimension to the output (matrix or array).

## Probability Forests

Setting `probability = TRUE` with a categorical dependent variable grows a probability forest. Here, the node impurity is used for splitting, as in classification forests. Predictions are class probabilities for each sample. In contrast to other implementations, each tree returns a probability estimate and these estimates are averaged for the forest probability estimate. For details see Malley et al. (2012). 

__Malley, J. D., Kruppa, J., Dasgupta, A., Malley, K. G., & Ziegler, A. (2012). Probability machines: consistent probability estimation using nonparametric learning machines. Methods Inf Med 51:74-81. https://doi.org/10.3414/ME00-01-0052__ 

Slides:
https://bigdata.unl.edu/documents/ASA_Workshop_Materials/Probability%20Machines.pdf

* Mostly theoretical.
* P25 says tuning adjusts to sample size e.g. size k of smallest terminal node for decision tree should have k/log(n) go to infinity as n goes to infinity.
* Any nonparametric regression machine that is provably consistent is a probability machine.
* Nearest neighbours and Random forest using random splits are both universally strongly consistent.
* The prob option in randomForest has unknown consistency properties.
* SVM not generally consistent.
* ROC score is not a good way to validate prob machine. 
* If a probability machine is consistent then the Brier score is also consistent.

There are further papers given in the JSS ranger paper: https://arxiv.org/pdf/1508.04409.pdf

__Making Sense of Random Forest Probabilities: a Kernel Perspective (2018) Matthew Olson Abraham J. Wyner https://arxiv.org/abs/1812.05792__ 

* Says that consistency of prob estimates depends on hyperparameters. Gives example which is clearly not consistent for low mtry.
* It does mention earlier theory, but says it does not give guarantees. IDK exactly what the theory says but it clearly does not hold in the example.
* “tuning parameters are critical in practice”

For binary targets, probability prediction should be very similar to regression on 0/1. A practical difference is that the default `min.node.size` is: _1 for classification, 5 for regression, 3 for survival, and 10 for probability_.

https://stats.stackexchange.com/questions/226109/how-does-predict-randomforest-estimate-class-probabilities says prob estimates are different for randomForest and ranger:

>For ranger...each tree predicts class probabilities and these probabilities are averaged for the forest prediction. For two classes, this is equivalent to a regression forest on a 0-1 coded response.
...in randomForest with type="prob" each tree predicts a class and probabilities are calculated from these classes.

## Interpretation

[Slides on RF variable importance issues](http://www.statistik.uni-dortmund.de/useR-2008/slides/Strobl+Zeileis.pdf) - has lots of references. 

[Thread - Obtaining knowledge from a random forest](https://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest)

[randomForestExplainer package](https://cran.rstudio.com/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html).

[Blog post using the DALEX package](https://olgamie.github.io/2020/09/30/building-first-baseline-with-random-forest-using-ranger-and-dalex/)

[Datacamp article on the Boruta package](https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)

[Boruta for those in a hurry](https://cran.r-project.org/web/packages/Boruta/vignettes/inahurry.pdf)

### Tree Information

Each package has functions to extract information about each tree such as which variable was split on, where the split was and the prediction for each terminal node.

For `randomForest` there is the function `getTree(rfobj, k=1, labelVar=FALSE)` where `k` gives the tree number. 
If `labelVar=FALSE` then a matrix is returned while `labelVar=TRUE` a data frame is returned with a named column `split var` instead of a number.  

`ranger` has `treeInfo(object, tree = 1)` which returns a data frame.

## Categorical Variable Handling

The best information on this is in this 2019 paper [Splitting on categorical predictors in random forests](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6368971/). One of the authors is the author of the `ranger` package. 

For unordered categorical variables there are $2^{k-1}-1$ ways of splitting into two partitions. As well as taking a long time to find, there is also the problem of encoding the split. This can be done by recording a binary number with each digit a 0 or 1 depending on which branch the corresponding level is assigned to. `randomForest` and `ranger` stores this as a double which gives a maximum limit of 53 levels per variable. Options to work around this are to treat the variable as ordered ($k-1$ possible splits), bin levels in some way, or use dummy variables (also called one hot encoding). The last two methods generally give inferior fitting models, as does generic ordering such as alphabetic. 

There are other ways of ordering the levels. For regression and binary classification, categorical factors can be arranged in order of mean response and treated as ordered while still choosing the optimal split (in gini/mse terms). These results don't hold for multivariate targets.

I am unclear of the extent of this result though. The paper makes the distinction between ordering once before any splitting, and ordering before each split. The former is obviously easier and quicker while the second, although faster than full partition enumeration, still requires the same long number to record the split and hence the level limit. I think the paper says the results only hold with reordering with each split but it's not totally clear. Why would the orderings be different with each split? They would be different for each tree because of the different bootstrap samples but this wouldn't change with the splits. I suppose that each split is effectively a subset of the data so this would change the ordering.

The `randomForest` package uses every split ordering for regression and binary classification and all partitions for multiclass classification. I am not certain if this ordering is by mean response or random.

The `ranger` approach has three options for handling unordered factors, using the argument `respect.unordered.factors`. This can be set to one of 'ignore', 'order' and 'partition' (or use TRUE (='order') or FALSE (='ignore')). 

+ `ignore` or `FALSE` is the default and treats the factors as ordered (ordering by usual alphanumeric rules). 
+ `order` or `TRUE` uses the one time ordering by mean response.
+ `partition` uses the full combinatorial enumeration of possible splits.

The paper finds, in simulations and real data, that the one time ordering method is at least as good as the other alternatives in terms of error as well as being simpler and faster but, oddly, the `ranger` default setting is `ignore`, which is usually gives the worst results.

An earlier example (artificial) example of this is given in a [blog post](http://www.win-vector.com/blog/2016/05/on-ranger-respect-unordered-factors/). I ran the code given there with `respect.unordered.factors` set to TRUE, FALSE and 'partition', as well as `randomForest()` with standard settings (mtry=2 to match the ranger models). The mse prediction errors were:

```{r wv_results, echo=FALSE}
tibble(model = c("FALSE", "TRUE", 'partition', "randomForest"), 
       mse_oob = c(4.16, 0.84, 1.92, 2.53),
       mse_test = c(2.69, 1.15, 1.50, 3.20)) %>% 
  my_kable()
```

Although this is only a 100 x 4 data set with 20 levels in each variable, the partition ranger model took 195 seconds to run even with parallelisation. The other options were near instantaneous. This indicates that partition is unlikely to be practical. 

There are a few other discussions on the ranger github [here](https://github.com/imbs-hl/ranger/issues/36) and [here](https://github.com/mlr-org/mlr/pull/918), but these will have been superseded by the paper.

## Wide Data Benchmark

I had heard it suggested that random forests would not be appropriate for wide data so I ran a test. The following code will produce a wide dataset with 1000 rows and fit ranger models with 50 trees to it, using a single thread.

```
library(dplyr)
library(ranger)
library(glmnet)
nn <- 1e3
pp <- 2e4
xmat <- matrix(sample(c(TRUE, FALSE), nn * pp, replace = TRUE), ncol = pp)
nms <- paste0("x", 1:pp)
colnames(xmat) <- nms
y = rnorm(nn)
dt <- as_tibble(xmat) %>% 
  mutate(y = y) 

edwards::object_size_all()
system.time(ranger(y ~ ., data = dt, write.forest = FALSE, num.trees = 50, num.threads = 1))
system.time(ranger(x = xmat, y = y, write.forest = FALSE, num.trees = 50, num.threads = 1))
system.time(glm <- cv.glmnet(xmat, y, nfolds = 3))
```

The standard formula method failed with 20000 columns with message `Error: protect(): protection stack overflow`, even with 10 trees. This error is explained at https://stackoverflow.com/questions/32826906/how-to-solve-protection-stack-overflow-issue-in-r-studio. The problem will be the large object generated internally by `model.terms()`. This can be avoided in ranger by creating a model matrix first and passing `x` and `y` arguments to `ranger()` rather than formulas.

This solved the problem and gave the following timings in seconds. In practice, ranger would need more trees but I'm just interested in how the methods scale, not absolute timings.

|  p  | Ranger Formula | Ranger XY | Glmnet |
|:---:|:--------------:|:---------:|:------:|
| 1E4 |        8       |    1.5    |    4   |
| 2E4 |      Fail      |    2.5    |   11   |
| 4E4 |      Fail      |     4     |   21   |
| 8E4 |      Fail      |     7     |   41   |

## Misc

Ranger coerces characters to unordered factors but I cannot find this in the documentation.
For logicals it makes no difference whether logical, integer, factor or ordered.

`ranger` has built in parallel processing (implemented by default). For `randomForest` you need to use explicitly use foreach and doParallel or similar.
